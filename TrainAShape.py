import os
import json
import random
from typing import Callable, Dict, List, Tuple
import torch
import numpy as np
import DeepSDFStruct.deep_sdf.data as deep_data
import DeepSDFStruct.deep_sdf.training as training

# ------------------------
# Limit CPU threads globally
# unique to the  machine. Adjust number of CPU cores as needed
# Additionally deeoSDFStruct supports NVDIA GPU acceleration
# ------------------------

NUM_CORES = 16
os.environ["OMP_NUM_THREADS"] = str(NUM_CORES)
os.environ["MKL_NUM_THREADS"] = str(NUM_CORES)
torch.set_num_threads(NUM_CORES)
torch.set_num_interop_threads(min(4, NUM_CORES // 2))

# ------------------------
# Patch DeepSDF loader for flat SdfSamples
# ------------------------
def patch_get_instance_filenames():
    def get_instance_filenames(data_source, split):
        npyfiles = []
        for split_name, classes in split.items():
            for class_name, instances in classes.items():
                for instance_name in instances:
                    npyfiles.append(f"{instance_name}.npz")
        return npyfiles
    deep_data.get_instance_filenames = get_instance_filenames

patch_get_instance_filenames()

# ------------------------
# Train a model on shapes
# Must have function inputs and behave like a continuous SDF function
#
# Caution! 
# Upon resuming training this script generates log.pth files to satisfy DeepSDF's API. 
#
# the log.pth generated by the script are dummy files and do not contain any logs
#
# Any such useful informtion could be extracted from this script with
# minor modifications however. 
# ------------------------

SDFCallable = Callable[[torch.Tensor, torch.Tensor | None], torch.Tensor]
def trainAShape(
    base_directory: str,
    model_name: str,
    scenes: Dict[str, Dict[int, Tuple[SDFCallable, List[Tuple[float, float]]]]],
    resume: bool = True,
    domainRadius: float = 1.0,
    latentDim: int = 1,
    FORCE_ONLY_FINAL_SNAPSHOT: bool = False
):

    # ------------- derive global param space from scenes -------------
    global_add_param_ranges = []
    scene_names = list(scenes.keys())
    global_add_num_params = []

    for scene in scenes.values():
        scene_param_ranges = []
        i =0
        for op_params in scene.values():
            if isinstance(op_params, (list, tuple)):
                for rng in op_params:
                    if isinstance(rng, (list, tuple)) and len(rng) == 2:
                        lo, hi = rng
                        if hi >= lo:
                            scene_param_ranges.append((lo, hi))
            i+=1
        scene_add_n_params = len(scene_param_ranges)/len(scene.values())
        if scene_add_n_params != int(scene_add_n_params):
            raise ValueError("Inconsistent number of parameters across operators in scene" + scene_names[i])
        
        if len(scene.values()) >1: scene_add_n_params +=1 

        for i in range(0,len(scene_param_ranges)-1):
            if (scene_param_ranges[i] != scene_param_ranges[i+1]):
                raise Warning(f"Inconsistent range of parameters between operators : {i} and {i+1} probably not the best for training")
            
        global_add_param_ranges.append(scene_param_ranges)
        global_add_num_params.append(int(scene_add_n_params))

    for i in range(0,len(global_add_param_ranges)-1):
        if (global_add_param_ranges[i].__len__() != global_add_param_ranges[i+1].__len__()):
            raise ValueError(f"Inconsistent number of parameters between scenes : {list(scenes.keys())[i]} and {list(scenes.keys())[i+1]} probably not the best for training")
        
        if (global_add_param_ranges[i] != global_add_param_ranges[i+1]):
            raise ValueError(f"Inconsistent range of parameters between scenes : {list(scenes.keys())[i]} and {list(scenes.keys())[i+1]} probably not the best for training")
    
    geom_n_params = global_add_num_params[0] +3 # all scenes must have same number of params

    
    
    # ---------------- Folder setup ----------------
    root = os.path.abspath(os.path.join(base_directory, model_name))  # absolute path
    split_dir = os.path.abspath(os.path.join(root, "split"))
    model_params_dir = os.path.abspath(os.path.join(root, "ModelParameters"))
    scenes_dir = os.path.abspath(os.path.join(root, "Scenes"))
    samples_dir = os.path.abspath(os.path.join(root, "SdfSamples"))
    top_latent_dir = os.path.abspath(os.path.join(root, "LatentCodes"))

    for d in [root, split_dir, model_params_dir, scenes_dir, samples_dir, top_latent_dir]:
        os.makedirs(d, exist_ok=True)

    print(f"[DEBUG] Using experiment directory: {root}")
    print(f"[DEBUG] Latent code directory: {top_latent_dir}")

    # ---------------- Specs ----------------
    specs_path = os.path.join(root, "specs.json")
    if os.path.exists(specs_path):
        with open(specs_path) as f:
            specs = json.load(f)
    else:
        specs = {
            "Description": f"Train DeepSDF on analytic {model_name} shapes.",
            "NetworkArch": "deep_sdf_decoder",
            "DataSource": root,
            "TrainSplit": "split/TrainSplit.json",
            "NetworkSpecs": {
                "dims": [128]*6,
                "dropout": list(range(6)),
                "dropout_prob": 0.2,
                "norm_layers": list(range(6)),
                "latent_in": [2] if latentDim > 0 else [],
                "xyz_in_all": False,
                "use_tanh": False,
                "latent_dropout": False,
                "weight_norm": True,
                "geom_dimension": geom_n_params
            },
            "CodeLength": latentDim,
            "NumEpochs": 500,
            "SnapshotFrequency": 1,
            "AdditionalSnapshots": [1, 5],
            "LearningRateSchedule": [
                {"Type": "Step", "Initial": 0.001, "Interval": 250, "Factor": 0.5},
                {"Type": "Constant", "Value": 0.001}
            ],
            "SamplesPerScene": 5000,
            "ScenesPerBatch": 1,
            "DataLoaderThreads": 1,
            "ClampingDistance": 0.1,
            "CodeRegularization": True,
            "CodeRegularizationLambda": 1e-4,
            "CodeBound": 1.0
        }

    if FORCE_ONLY_FINAL_SNAPSHOT:
        specs["SnapshotFrequency"] = specs["NumEpochs"]
        specs["AdditionalSnapshots"] = [specs["NumEpochs"]]

    with open(specs_path, "w") as f:
        json.dump(specs, f, indent=2)

    # ---------------- TrainSplit.json ----------------
    train_split_path = os.path.join(split_dir, "TrainSplit.json")
    if os.path.exists(train_split_path):
        with open(train_split_path) as f:
            split_dict = json.load(f)
    else:
        split_dict = {"train": {}}
    split_dict.setdefault("train", {}).setdefault(model_name, [])
    for scene_id in scenes.keys():
        key = f"{model_name.lower()}_{scene_id:03d}"
        if key not in split_dict["train"][model_name]:
            split_dict["train"][model_name].append(key)
    with open(train_split_path, "w") as f:
        json.dump(split_dict, f, indent=2)

    # ---------------- Training Loop ----------------
    for scene_idx, (scene_id, sdf_parameters) in enumerate(scenes.items()):
        scene_key = f"{model_name.lower()}_{scene_id:03d}"
        scene_folder = os.path.join(scenes_dir, f"{scene_id:03d}")
        scene_latent_dir = os.path.join(scene_folder, "LatentCodes")
        os.makedirs(scene_latent_dir, exist_ok=True)

        # ---------------- Resume logic: find highest checkpoint ----------------
        existing_ckpts = [
            f for f in os.listdir(top_latent_dir)
            if f.endswith(".pth") and f[:-4].isdigit()
        ]
        latest_epoch = max([int(f[:-4]) for f in existing_ckpts], default=0)

        if resume and latest_epoch >= specs["NumEpochs"]:
            print(f"[INFO] Scene {scene_key} already fully trained (epoch {latest_epoch}), skipping...")
            continue

        resume_ckpt = None
        if resume and latest_epoch > 0:
            resume_ckpt = str(latest_epoch)
            print(f"[INFO] Resuming {model_name} at epoch {latest_epoch}")

            latent_path = os.path.join(top_latent_dir, f"{resume_ckpt}.pth")
            print(f"[DEBUG] Expecting latent file at: {os.path.abspath(latent_path)}")
            print(f"[DEBUG] Exists: {os.path.exists(latent_path)}")
        else:
            print(f"[INFO] Starting fresh training for {model_name}")

        # --- SDF Samples (fixed) ---
        samples_file = os.path.join(samples_dir, f"{scene_key}.npz")
        if not os.path.exists(samples_file):
            sample_points_per_operator = 50000

            # ---------------- Operator setup ----------------
            operator_keys = list(sdf_parameters.keys())
            random.shuffle(operator_keys)  # shuffle operators
            n_ops = len(operator_keys)

            param_ranges_flat = global_add_param_ranges[scene_idx]
            add_n_params_not_operator = global_add_num_params[scene_idx]

            # ---------------- Sample XYZ coordinates ----------------
            xyz_all = (torch.rand(sample_points_per_operator, 3) * 2 - 1) * domainRadius

            # ---------------- Prepare param ranges per operator ----------------
            lows = torch.tensor([
                [lo for lo, _ in param_ranges_flat[i*add_n_params_not_operator:(i+1)*add_n_params_not_operator]]
                for i in range(n_ops)
            ], dtype=torch.float32)

            highs = torch.tensor([
                [hi for _, hi in param_ranges_flat[i*add_n_params_not_operator:(i+1)*add_n_params_not_operator]]
                for i in range(n_ops)
            ], dtype=torch.float32)

            # ---------------- Allocate storage ----------------
            queries_list = []
            sdf_vals_list = []

            # ---------------- Sample per operator ----------------
            for i, key in enumerate(operator_keys):
                # sample parameters for all points for this operator
                rand_params = torch.rand((sample_points_per_operator, add_n_params_not_operator), dtype=torch.float32)
                sampled_params = lows[i] + rand_params * (highs[i] - lows[i])

                # assemble queries: [x, y, z, operator_key_as_float?, params...]
                # optionally encode operator as float or just leave out if not needed
                op_value = torch.full((sample_points_per_operator, 1), float(i))  # optional
                queries_op = torch.cat([xyz_all, op_value, sampled_params], dim=1)

                # evaluate SDF
                sdf_fn, _ = sdf_parameters[key]
                sdf_op_vals = sdf_fn(xyz_all, sampled_params)

                queries_list.append(queries_op)
                sdf_vals_list.append(sdf_op_vals.unsqueeze(1))

            # ---------------- Combine all operators ----------------
            queries = torch.cat(queries_list, dim=0)
            sdf_vals = torch.cat(sdf_vals_list, dim=0)

            # ---------------- Clamping ----------------
            data = torch.cat([queries, sdf_vals], dim=1).numpy()
            sdf_col_idx = data.shape[1] - 1

            pos = data[np.abs(data[:, sdf_col_idx]) < specs["ClampingDistance"]]
            neg = data[np.abs(data[:, sdf_col_idx]) >= specs["ClampingDistance"]]

            n_pos = min(len(pos), specs["SamplesPerScene"] // 2)
            n_neg = specs["SamplesPerScene"] - n_pos

            if len(pos) > 0:
                pos = pos[np.random.choice(len(pos), n_pos, replace=len(pos) < n_pos)]
            else:
                pos = np.empty((0, data.shape[1]), dtype=data.dtype)

            if len(neg) > 0:
                neg = np.random.choice(len(neg), n_neg, replace=len(neg) < n_neg)
            else:
                neg = np.empty((0, data.shape[1]), dtype=data.dtype)

            # ---------------- Save ----------------
            np.savez_compressed(samples_file, pos=pos, neg=neg)



        # ---------------- Train ----------------
        old_cwd = os.getcwd()
        os.chdir(root)
        try:
            print(f"[DEBUG] Changed directory to: {os.getcwd()}")

            logs_file = os.path.join(root, "Logs.pth")
            if not os.path.exists(logs_file):
                print(f"[WARN] Logs.pth not found — creating a minimal log so training can resume.")
                torch.save(
                    {
                        "loss": [0.0],              # at least 1 element
                        "learning_rate": [0.001],   # at least 1 element
                        "timing": [0.0],
                        "latent_magnitude": [0.0],
                        "param_magnitude": {"dummy": [0.0]},
                        "epoch": [0],               # 0 is fine for start
                    },
                    logs_file,
                )

            # --- Run training (resume if resume_ckpt is valid) ---
            training.train_deep_sdf(
                experiment_directory=root,
                data_source=root,
                continue_from=resume_ckpt,
                batch_split=1
            )

        finally:
            os.chdir(old_cwd)
            print(f"[DEBUG] Restored directory to: {os.getcwd()}")

        # ---------------- Find latest (highest) checkpoint after training ----------------
        post_ckpts = [
            f for f in os.listdir(top_latent_dir)
            if f.endswith(".pth") and f[:-4].isdigit()
        ]
        if not post_ckpts:
            raise RuntimeError(f"No checkpoints found in {top_latent_dir} after training.")
        final_epoch = max(int(f[:-4]) for f in post_ckpts)

        # ---------------- Extract trained latent vector ----------------
        final_ckpt_file = os.path.join(top_latent_dir, f"{final_epoch}.pth")
        top_data = torch.load(final_ckpt_file, map_location="cpu")
        latent_trained = top_data["latent_codes"][scene_key]

        # ---------------- Save per-scene snapshots of epochs ----------------
        final_scene_file = os.path.join(scene_latent_dir, f"{final_epoch}.pth")
        torch.save(
            {"latent_codes": {scene_key: latent_trained}, "epochs": {scene_key: final_epoch}},
            final_scene_file
        )

        print(f"[INFO] Finished training scene {scene_key} (final epoch {final_epoch})")