import os
import json
import torch
import numpy as np
import DeepSDFStruct.deep_sdf.data as deep_data
import DeepSDFStruct.deep_sdf.training as training

# ------------------------
# Limit CPU threads globally
# unique to the  machine. Adjust number of CPU cores as needed
# Additionally deeoSDFStruct supports NVDIA GPU acceleration
# ------------------------

NUM_CORES = 16
os.environ["OMP_NUM_THREADS"] = str(NUM_CORES)
os.environ["MKL_NUM_THREADS"] = str(NUM_CORES)
torch.set_num_threads(NUM_CORES)
torch.set_num_interop_threads(min(4, NUM_CORES // 2))

# ------------------------
# Patch DeepSDF loader for flat SdfSamples
# ------------------------
def patch_get_instance_filenames():
    def get_instance_filenames(data_source, split):
        npyfiles = []
        for split_name, classes in split.items():
            for class_name, instances in classes.items():
                for instance_name in instances:
                    npyfiles.append(f"{instance_name}.npz")
        return npyfiles
    deep_data.get_instance_filenames = get_instance_filenames

patch_get_instance_filenames()

# ------------------------
# Train a model on shapes
# Must have function inputs and behave like a continuous SDF function
#
# Caution! 
# Upon resuming training this script generates log.pth files to satisfy DeepSDF's API. 
#
# the log.pth generated by the script are dummy files and do not contain any logs
#
# Any such useful informtion could be extracted from this script with
# minor modifications however. 
# ------------------------
def trainAShape(
    base_directory: str,
    model_name,
    sdf_function,
    scene_ids,
    resume=True,
    domainRadius=1.0,
    sdf_parameters=None,
    latentDim=1,
    FORCE_ONLY_FINAL_SNAPSHOT=False
):
    # Normalize global sdf_parameters (for specs)
    n_params = 0
    if sdf_parameters is None:
        sdf_parameters = []
    param_names = []
    param_ranges = {}
    if isinstance(sdf_parameters, dict):
        param_names = list(sdf_parameters.keys())
        param_ranges = dict(sdf_parameters)  # preserve original before flattening

        valid = {}
        for k, v in param_ranges.items():
            if isinstance(v, (list, tuple)) and len(v) == 2:
                lo, hi = v
                if isinstance(lo, (int, float)) and isinstance(hi, (int, float)) and hi > lo:
                    valid[k] = (lo, hi)
        param_ranges = valid

        # flatten for internal sampling if needed
        sdf_parameters = list(param_ranges.values())

        n_params = len(sdf_parameters)  # number of sampled parameters for this model

    
    # ---------------- Folder setup ----------------
    root = os.path.abspath(os.path.join(base_directory, model_name))  # absolute path
    split_dir = os.path.abspath(os.path.join(root, "split"))
    model_params_dir = os.path.abspath(os.path.join(root, "ModelParameters"))
    scenes_dir = os.path.abspath(os.path.join(root, "Scenes"))
    samples_dir = os.path.abspath(os.path.join(root, "SdfSamples"))
    top_latent_dir = os.path.abspath(os.path.join(root, "LatentCodes"))

    for d in [root, split_dir, model_params_dir, scenes_dir, samples_dir, top_latent_dir]:
        os.makedirs(d, exist_ok=True)

    print(f"[DEBUG] Using experiment directory: {root}")
    print(f"[DEBUG] Latent code directory: {top_latent_dir}")

    # ---------------- Specs ----------------
    specs_path = os.path.join(root, "specs.json")
    if os.path.exists(specs_path):
        with open(specs_path) as f:
            specs = json.load(f)
    else:
        specs = {
            "Description": f"Train DeepSDF on analytic {model_name} shapes.",
            "NetworkArch": "deep_sdf_decoder",
            "DataSource": root,
            "TrainSplit": "split/TrainSplit.json",
            "NetworkSpecs": {
                "dims": [128]*6,
                "dropout": list(range(6)),
                "dropout_prob": 0.2,
                "norm_layers": list(range(6)),
                "latent_in": [2],
                "xyz_in_all": False,
                "use_tanh": False,
                "latent_dropout": False,
                "weight_norm": True,
                "geom_dimension": n_params + 3
            },
            "CodeLength": latentDim,
            "NumEpochs": 500,
            "SnapshotFrequency": 1,
            "AdditionalSnapshots": [1, 5],
            "LearningRateSchedule": [
                {"Type": "Step", "Initial": 0.001, "Interval": 250, "Factor": 0.5},
                {"Type": "Constant", "Value": 0.001}
            ],
            "SamplesPerScene": 5000,
            "ScenesPerBatch": 1,
            "DataLoaderThreads": 1,
            "ClampingDistance": 0.1,
            "CodeRegularization": True,
            "CodeRegularizationLambda": 1e-4,
            "CodeBound": 1.0,
            "ParamNames": param_names,
            "ParamRanges": param_ranges
        }

    if FORCE_ONLY_FINAL_SNAPSHOT:
        specs["SnapshotFrequency"] = specs["NumEpochs"]
        specs["AdditionalSnapshots"] = [specs["NumEpochs"]]

    with open(specs_path, "w") as f:
        json.dump(specs, f, indent=2)

    # ---------------- TrainSplit.json ----------------
    train_split_path = os.path.join(split_dir, "TrainSplit.json")
    if os.path.exists(train_split_path):
        with open(train_split_path) as f:
            split_dict = json.load(f)
    else:
        split_dict = {"train": {}}
    split_dict.setdefault("train", {}).setdefault(model_name, [])
    for scene_id in scene_ids:
        key = f"{model_name.lower()}_{scene_id:03d}"
        if key not in split_dict["train"][model_name]:
            split_dict["train"][model_name].append(key)
    with open(train_split_path, "w") as f:
        json.dump(split_dict, f, indent=2)

    # ---------------- Training Loop ----------------
    for scene_id in scene_ids:
        scene_key = f"{model_name.lower()}_{scene_id:03d}"
        scene_folder = os.path.join(scenes_dir, f"{scene_id:03d}")
        scene_latent_dir = os.path.join(scene_folder, "LatentCodes")
        os.makedirs(scene_latent_dir, exist_ok=True)

        # ---------------- Resume logic: find highest checkpoint ----------------
        existing_ckpts = [
            f for f in os.listdir(top_latent_dir)
            if f.endswith(".pth") and f[:-4].isdigit()
        ]
        latest_epoch = max([int(f[:-4]) for f in existing_ckpts], default=0)

        if resume and latest_epoch >= specs["NumEpochs"]:
            print(f"[INFO] Scene {scene_key} already fully trained (epoch {latest_epoch}), skipping...")
            continue

        resume_ckpt = None
        if resume and latest_epoch > 0:
            resume_ckpt = str(latest_epoch)
            print(f"[INFO] Resuming {model_name} at epoch {latest_epoch}")

            latent_path = os.path.join(top_latent_dir, f"{resume_ckpt}.pth")
            print(f"[DEBUG] Expecting latent file at: {os.path.abspath(latent_path)}")
            print(f"[DEBUG] Exists: {os.path.exists(latent_path)}")
        else:
            print(f"[INFO] Starting fresh training for {model_name}")

        # ---------------- SDF Samples ----------------
        # Maximizes positive samples until 50/50 split between positive/negative
        # This can be adjusted as needed but is theoretically ideal for training

        samples_file = os.path.join(samples_dir, f"{scene_key}.npz")
        if not os.path.exists(samples_file):
            n_points = 50_000

            # Flatten sdf_parameters if passed as list of dicts
            param_ranges = []
            if sdf_parameters:
                for p in sdf_parameters:
                    if isinstance(p, dict):
                        for v in p.values():
                            if isinstance(v, (list, tuple)) and len(v) == 2:
                                lo, hi = v
                                if hi > lo:
                                    param_ranges.append((lo, hi))
                    elif isinstance(p, (list, tuple)) and len(p) == 2:
                        lo, hi = p
                        if hi > lo:
                            param_ranges.append((lo, hi))

            n_params = len(param_ranges)
            queries = torch.empty(n_points, 3 + n_params)

            # sample xyz in bounding cube
            queries[:, :3] = (torch.rand(n_points, 3) * 2 - 1) * domainRadius

            # sample parameters
            if n_params > 0:
                lows  = torch.tensor([lo for lo, hi in param_ranges], dtype=torch.float32)
                highs = torch.tensor([hi for lo, hi in param_ranges], dtype=torch.float32)
                param_rand = torch.rand(n_points, n_params)
                queries[:, 3:] = lows + param_rand * (highs - lows)

            # evaluate sdf
            sdf_vals = sdf_function(queries).squeeze(1)
            data = torch.cat([queries, sdf_vals.unsqueeze(1)], dim=1).numpy()

            # separate positive/negative samples
            idx = 3 + n_params
            pos = data[np.abs(data[:, idx]) < specs["ClampingDistance"]]
            neg = data[np.abs(data[:, idx]) >= specs["ClampingDistance"]]
            n_pos = min(len(pos), specs["SamplesPerScene"] // 2)
            n_neg = specs["SamplesPerScene"] - n_pos
            if len(pos) > 0:
                pos = pos[np.random.choice(len(pos), n_pos, replace=False)]
            if len(neg) > 0:
                neg = neg[np.random.choice(len(neg), n_neg, replace=False)]

            np.savez_compressed(samples_file, pos=pos, neg=neg)

        # ---------------- Train ----------------
        old_cwd = os.getcwd()
        os.chdir(root)
        try:
            print(f"[DEBUG] Changed directory to: {os.getcwd()}")

            logs_file = os.path.join(root, "Logs.pth")
            if not os.path.exists(logs_file):
                print(f"[WARN] Logs.pth not found â€” creating a minimal log so training can resume.")
                torch.save(
                    {
                        "loss": [0.0],              # at least 1 element
                        "learning_rate": [0.001],   # at least 1 element
                        "timing": [0.0],
                        "latent_magnitude": [0.0],
                        "param_magnitude": {"dummy": [0.0]},
                        "epoch": [0],               # 0 is fine for start
                    },
                    logs_file,
                )

            # --- Run training (resume if resume_ckpt is valid) ---
            training.train_deep_sdf(
                experiment_directory=root,
                data_source=root,
                continue_from=resume_ckpt,
                batch_split=1
            )

        finally:
            os.chdir(old_cwd)
            print(f"[DEBUG] Restored directory to: {os.getcwd()}")

        # ---------------- Find latest (highest) checkpoint after training ----------------
        post_ckpts = [
            f for f in os.listdir(top_latent_dir)
            if f.endswith(".pth") and f[:-4].isdigit()
        ]
        if not post_ckpts:
            raise RuntimeError(f"No checkpoints found in {top_latent_dir} after training.")
        final_epoch = max(int(f[:-4]) for f in post_ckpts)

        # ---------------- Extract trained latent vector ----------------
        final_ckpt_file = os.path.join(top_latent_dir, f"{final_epoch}.pth")
        top_data = torch.load(final_ckpt_file, map_location="cpu")
        latent_trained = top_data["latent_codes"][scene_key]

        # ---------------- Save per-scene snapshots of epochs ----------------
        final_scene_file = os.path.join(scene_latent_dir, f"{final_epoch}.pth")
        torch.save(
            {"latent_codes": {scene_key: latent_trained}, "epochs": {scene_key: final_epoch}},
            final_scene_file
        )

        print(f"[INFO] Finished training scene {scene_key} (final epoch {final_epoch})")