import os
import json
import random
from typing import Callable, Dict, List, Tuple
import torch
import numpy as np
import DeepSDFStruct.deep_sdf.data as deep_data
import DeepSDFStruct.deep_sdf.training as training

# ------------------------
# Limit CPU threads globally
# unique to the  machine. Adjust number of CPU cores as needed
# Additionally deeoSDFStruct supports NVDIA GPU acceleration
# ------------------------

NUM_CORES = 16
os.environ["OMP_NUM_THREADS"] = str(NUM_CORES)
os.environ["MKL_NUM_THREADS"] = str(NUM_CORES)
torch.set_num_threads(NUM_CORES)
torch.set_num_interop_threads(min(4, NUM_CORES // 2))

# ------------------------
# Patch DeepSDF loader for flat SdfSamples
# ------------------------
def patch_get_instance_filenames():
    def get_instance_filenames(data_source, split):
        npyfiles = []
        for split_name, classes in split.items():
            for class_name, instances in classes.items():
                for instance_name in instances:
                    npyfiles.append(f"{instance_name}.npz")
        return npyfiles
    deep_data.get_instance_filenames = get_instance_filenames

patch_get_instance_filenames()

# ------------------------
#
# Caution! 
# Upon resuming training this script generates log.pth files to satisfy DeepSDF's API. 
#
# the log.pth generated by the script are dummy files and do not contain any logs
#
# Any such useful informtion could be extracted from this script with
# minor modifications however. 
# ------------------------

SDFCallable = Callable[[torch.Tensor, torch.Tensor | None], torch.Tensor]
def trainAShape(
    base_directory: str,
    model_name: str,
    scenes: Dict[str, Dict[int, Tuple[SDFCallable, List[Tuple[float, float]]]]],
    resume: bool = True,
    domainRadius: float = 1.0,
    latentDim: int = 1,
    FORCE_ONLY_FINAL_SNAPSHOT: bool = False
):
    """
    Train a DeepSDF model on SDFs with compatibility to interpolate between SDFs.
    Interpolation is achieved via operator parameters. 
    Scenes and operators for a scene are wrapped to contain additional input parameters beyond xyz.
    The only condition for compatibility is that all scenes and operators must have the same number of additional parameters.
    Additionally the model should only be trained on watertight smooth shapes for best results.
    If only one opertor is provided per scene the operator code input is omitted.
    """

    # ------------- derive global param space from scenes -------------
    global_add_param_ranges = []
    scene_names = list(scenes.keys())
    global_add_num_params = []

    for scene in scenes.values():
        scene_param_ranges = []
        i =0
        for op_params in scene.values():
            if isinstance(op_params, (list, tuple)):
                for rng in op_params:
                    if isinstance(rng, (list, tuple)) and len(rng) == 2:
                        lo, hi = rng
                        if hi >= lo:
                            scene_param_ranges.append((lo, hi))
            i+=1
        scene_add_n_params = len(scene_param_ranges)/len(scene.values())
        if scene_add_n_params != int(scene_add_n_params):
            raise ValueError("Inconsistent number of parameters across operators in scene" + scene_names[i])
        
        if len(scene.values()) >1: scene_add_n_params +=1 

        for i in range(0,len(scene_param_ranges)-1):
            if (scene_param_ranges[i] != scene_param_ranges[i+1]):
                raise Warning(f"Inconsistent range of parameters between operators : {i} and {i+1} probably not the best for training")
            
        global_add_param_ranges.append(scene_param_ranges)
        global_add_num_params.append(int(scene_add_n_params))

    for i in range(0,len(global_add_param_ranges)-1):
        if (global_add_param_ranges[i].__len__() != global_add_param_ranges[i+1].__len__()):
            raise ValueError(f"Inconsistent number of parameters between scenes : {list(scenes.keys())[i]} and {list(scenes.keys())[i+1]} probably not the best for training")
        
        if (global_add_param_ranges[i] != global_add_param_ranges[i+1]):
            raise ValueError(f"Inconsistent range of parameters between scenes : {list(scenes.keys())[i]} and {list(scenes.keys())[i+1]} probably not the best for training")
    
    geom_n_params = global_add_num_params[0] +3 # all scenes must have same number of params

    
    
    # ---------------- Folder setup ----------------
    root = os.path.abspath(os.path.join(base_directory, model_name))  # absolute path
    split_dir = os.path.abspath(os.path.join(root, "split"))
    model_params_dir = os.path.abspath(os.path.join(root, "ModelParameters"))
    scenes_dir = os.path.abspath(os.path.join(root, "Scenes"))
    samples_dir = os.path.abspath(os.path.join(root, "SdfSamples"))
    top_latent_dir = os.path.abspath(os.path.join(root, "LatentCodes"))

    for d in [root, split_dir, model_params_dir, scenes_dir, samples_dir, top_latent_dir]:
        os.makedirs(d, exist_ok=True)

    print(f"[DEBUG] Using experiment directory: {root}")
    print(f"[DEBUG] Latent code directory: {top_latent_dir}")

    # ---------------- Specs ----------------
    specs_path = os.path.join(root, "specs.json")
    if os.path.exists(specs_path):
        with open(specs_path) as f:
            specs = json.load(f)
    else:
        specs = {
            "Description": f"Train DeepSDF on analytic {model_name} shapes.",
            "NetworkArch": "deep_sdf_decoder",
            "DataSource": root,
            "TrainSplit": "split/TrainSplit.json",
            "NetworkSpecs": {
                "dims": [128]*6,
                "dropout": list(range(6)),
                "dropout_prob": 0.2,
                "norm_layers": list(range(6)),
                "latent_in": [2] if latentDim > 0 else [],
                "xyz_in_all": False,
                "use_tanh": False,
                "latent_dropout": False,
                "weight_norm": True,
                "geom_dimension": geom_n_params
            },
            "CodeLength": latentDim,
            "NumEpochs": 500,
            "SnapshotFrequency": 1,
            "AdditionalSnapshots": [1, 5],
            "LearningRateSchedule": [
                {"Type": "Step", "Initial": 0.001, "Interval": 250, "Factor": 0.5},
                {"Type": "Constant", "Value": 0.001}
            ],
            "SamplesPerScene": 5000,
            "ScenesPerBatch": 1,
            "DataLoaderThreads": 1,
            "ClampingDistance": 0.1,
            "CodeRegularization": True,
            "CodeRegularizationLambda": 1e-4,
            "CodeBound": 1.0
        }

    if FORCE_ONLY_FINAL_SNAPSHOT:
        specs["SnapshotFrequency"] = specs["NumEpochs"]
        specs["AdditionalSnapshots"] = [specs["NumEpochs"]]

    with open(specs_path, "w") as f:
        json.dump(specs, f, indent=2)

    # ---------------- TrainSplit.json ----------------
    train_split_path = os.path.join(split_dir, "TrainSplit.json")
    if os.path.exists(train_split_path):
        with open(train_split_path) as f:
            split_dict = json.load(f)
    else:
        split_dict = {"train": {}}
    split_dict.setdefault("train", {}).setdefault(model_name, [])
    for scene_id in scenes.keys():
        key = f"{model_name.lower()}_{scene_id:03d}"
        if key not in split_dict["train"][model_name]:
            split_dict["train"][model_name].append(key)
    with open(train_split_path, "w") as f:
        json.dump(split_dict, f, indent=2)

    # ---------------- Training Loop ----------------
    for scene_idx, (scene_id, sdf_parameters) in enumerate(scenes.items()):
        scene_key = f"{model_name.lower()}_{scene_id:03d}"
        scene_folder = os.path.join(scenes_dir, f"{scene_id:03d}")
        scene_latent_dir = os.path.join(scene_folder, "LatentCodes")
        os.makedirs(scene_latent_dir, exist_ok=True)

        # ---------------- Resume logic: find highest checkpoint ----------------
        existing_ckpts = [
            f for f in os.listdir(top_latent_dir)
            if f.endswith(".pth") and f[:-4].isdigit()
        ]
        latest_epoch = max([int(f[:-4]) for f in existing_ckpts], default=0)

        if resume and latest_epoch >= specs["NumEpochs"]:
            print(f"[INFO] Scene {scene_key} already fully trained (epoch {latest_epoch}), skipping...")
            continue

        resume_ckpt = None
        if resume and latest_epoch > 0:
            resume_ckpt = str(latest_epoch)
            print(f"[INFO] Resuming {model_name} at epoch {latest_epoch}")

            latent_path = os.path.join(top_latent_dir, f"{resume_ckpt}.pth")
            print(f"[DEBUG] Expecting latent file at: {os.path.abspath(latent_path)}")
            print(f"[DEBUG] Exists: {os.path.exists(latent_path)}")
        else:
            print(f"[INFO] Starting fresh training for {model_name}")

        # --- SDF Samples (fixed) ---
        samples_file = os.path.join(samples_dir, f"{scene_key}.npz")
        if not os.path.exists(samples_file):

            # ---------------- Operator setup ----------------
            operator_keys = list(sdf_parameters.keys())
            random.shuffle(operator_keys)  # shuffle operators
            n_ops = len(operator_keys)

            param_ranges_flat = global_add_param_ranges[scene_idx]
            add_n_params_not_operator = global_add_num_params[scene_idx] -1 if n_ops >1 else global_add_num_params[scene_idx]

            # ---------------- Adaptive sampling per operator ----------------
            # Attempts to sample 50/50 split of inside/outside points per operator

            target_pos = target_neg = specs["SamplesPerScene"] // n_ops// 2
            pos_list, neg_list = [], []

            batch_size = 5000  # sample in batches to avoid memory blow-up
            max_attempts = 100  # safety to avoid infinite loops

            for i, key in enumerate(operator_keys):
                op_idx = list(sdf_parameters.keys()).index(key)  # find the index in param_ranges_flat
                low_vals = torch.tensor([lo for lo, _ in param_ranges_flat[op_idx]], dtype=torch.float32)
                high_vals = torch.tensor([hi for _, hi in param_ranges_flat[op_idx]], dtype=torch.float32)

                attempts = 0
                while (len(pos_list) < target_pos or len(neg_list) < target_neg) and attempts < max_attempts:
                    attempts += 1

                    # sample xyz
                    xyz = (torch.rand(batch_size, 3, dtype=torch.float32) * 2 - 1) * domainRadius

                    # sample operator parameters
                    rand_params = torch.rand((batch_size, add_n_params_not_operator), dtype=torch.float32)
                    sampled_params = low_vals + rand_params * (high_vals - low_vals)

                    # continuous operator code (column 4)
                    op_value = torch.full((batch_size, 1), float(i), dtype=torch.float32)

                    # assemble query: [x, y, z, op_code, params...]
                    queries_op = torch.cat([xyz, op_value, sampled_params], dim=1)

                    # evaluate SDF
                    sdf_fn, _ = sdf_parameters[key]
                    sdf_op_vals = sdf_fn(xyz, sampled_params)
                    if sdf_op_vals.dim() == 1:
                        sdf_op_vals = sdf_op_vals.unsqueeze(1)

                    # clamp SDF values ensures that we do not oversample far away points
                    # this is best for training since the goal is to interpolate between shapes 
                    # and shapes are encoded implicitly by the zero level set
                    # thus points far away from the surface are not very useful

                    data = torch.cat([queries_op, sdf_op_vals], dim=1).numpy()
                    sdf_col_idx = data.shape[1] - 1
                    batch_pos = data[np.abs(data[:, sdf_col_idx]) < specs["ClampingDistance"]]
                    batch_neg = data[np.abs(data[:, sdf_col_idx]) >= specs["ClampingDistance"]]

                    # append only up to target
                    if len(pos_list) < target_pos:
                        remaining = target_pos - len(pos_list)
                        pos_list.append(batch_pos[:remaining])
                    if len(neg_list) < target_neg:
                        remaining = target_neg - len(neg_list)
                        neg_list.append(batch_neg[:remaining])

            # ---------------- Combine all batches ----------------
            pos = np.vstack(pos_list) if pos_list else np.empty((0, batch_size), dtype=np.float32)
            neg = np.vstack(neg_list) if neg_list else np.empty((0, batch_size), dtype=np.float32)

            # Determine if all scenes have exactly one operator
            all_scenes_single_operator = all(len(scene) == 1 for scene in scenes.values())

            # ... inside your sampling loop, replace the previous trimming block with:
            if all_scenes_single_operator:
                pos = np.concatenate([pos[:, :3], pos[:, 4:]], axis=1)
                neg = np.concatenate([neg[:, :3], neg[:, 4:]], axis=1)
                
            # ---------------- Save ----------------
            np.savez_compressed(samples_file, pos=pos, neg=neg)

        # ---------------- Train ----------------
        old_cwd = os.getcwd()
        os.chdir(root)
        try:
            print(f"[DEBUG] Changed directory to: {os.getcwd()}")

            logs_file = os.path.join(root, "Logs.pth")
            if not os.path.exists(logs_file):
                print(f"[WARN] Logs.pth not found â€” creating a minimal log so training can resume.")
                torch.save(
                    {
                        "loss": [0.0],              # at least 1 element
                        "learning_rate": [0.001],   # at least 1 element
                        "timing": [0.0],
                        "latent_magnitude": [0.0],
                        "param_magnitude": {"dummy": [0.0]},
                        "epoch": [0],               # 0 is fine for start
                    },
                    logs_file,
                )

            # --- Run training (resume if resume_ckpt is valid) ---
            training.train_deep_sdf(
                experiment_directory=root,
                data_source=root,
                continue_from=resume_ckpt,
                batch_split=1
            )

        finally:
            os.chdir(old_cwd)
            print(f"[DEBUG] Restored directory to: {os.getcwd()}")

        # ---------------- Find latest (highest) checkpoint after training ----------------
        post_ckpts = [
            f for f in os.listdir(top_latent_dir)
            if f.endswith(".pth") and f[:-4].isdigit()
        ]
        if not post_ckpts:
            raise RuntimeError(f"No checkpoints found in {top_latent_dir} after training.")
        final_epoch = max(int(f[:-4]) for f in post_ckpts)

        # ---------------- Extract trained latent vector ----------------
        final_ckpt_file = os.path.join(top_latent_dir, f"{final_epoch}.pth")
        top_data = torch.load(final_ckpt_file, map_location="cpu")
        latent_trained = top_data["latent_codes"][scene_key]

        # ---------------- Save per-scene snapshots of epochs ----------------
        final_scene_file = os.path.join(scene_latent_dir, f"{final_epoch}.pth")
        torch.save(
            {"latent_codes": {scene_key: latent_trained}, "epochs": {scene_key: final_epoch}},
            final_scene_file
        )

        print(f"[INFO] Finished training scene {scene_key} (final epoch {final_epoch})")