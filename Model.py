import os
import json
import random
from typing import Callable, Dict, List, Optional, Tuple
import torch
import numpy as np
import DeepSDFStruct.deep_sdf.data as deep_data
import DeepSDFStruct.deep_sdf.training as training
from VisualizeAShape import visualize_a_shape
import multiprocessing
import warnings
import math
from DeepSDFStruct.deep_sdf.networks.deep_sdf_decoder import DeepSDFDecoder as Decoder

# ------------------------
# Limit CPU threads globally
# unique to the  machine. 
# Additionally deepSDFStruct and the original deepSDF support NVDIA GPU acceleration
# this has not been implemented in the script but would be useful
# ------------------------
NUM_CORES = multiprocessing.cpu_count()

# Set environment variables for parallel libraries
os.environ["OMP_NUM_THREADS"] = str(NUM_CORES)
os.environ["MKL_NUM_THREADS"] = str(NUM_CORES)

# Configure PyTorch threading
torch.set_num_threads(NUM_CORES)
torch.set_num_interop_threads(min(4, NUM_CORES // 2))

print(f"Configured PyTorch to use {NUM_CORES} CPU cores.")

# ------------------------
# Patch DeepSDF loader for flat SdfSamples
# ------------------------
def patch_get_instance_filenames():
    def get_instance_filenames(data_source, split):
        npyfiles = []
        for split_name, classes in split.items():
            for class_name, instances in classes.items():
                for instance_name in instances:
                    npyfiles.append(f"{instance_name}.npz")
        return npyfiles
    deep_data.get_instance_filenames = get_instance_filenames

patch_get_instance_filenames()

# ------------------------
# Caution! 
# Upon resuming training this script generates log.pth files to satisfy DeepSDF's API. 
#
# the log.pth generated by the script are dummy files and do not contain any logs
#
# Any such useful informtion could be extracted from this script with
# minor modifications however. 
# ------------------------
SDFCallable = Callable[[torch.Tensor, torch.Tensor | None], torch.Tensor]
SceneWithOperators = Dict[int, Tuple[SDFCallable, List[Tuple[float, float]]]]
Scenes = Dict[str, SceneWithOperators]

class Model: 
    def __init__(
        self,
        base_directory: str,
        model_name: str,
        scenes: Scenes,
        resume: bool = True,
        domainRadius: float = 1.0,
        latentDim: int = 1,
        FORCE_ONLY_FINAL_SNAPSHOT: bool = False,
        NumEpochs: int =500,
        ScenesPerBatch: int = 0,
    ):
        self.base_directory = base_directory
        self.model_name = model_name
        self.scenes = scenes
        self.resume = resume
        self.domainRadius = domainRadius
        self.latentDim = latentDim
        self.FORCE_ONLY_FINAL_SNAPSHOT = FORCE_ONLY_FINAL_SNAPSHOT
        self.trained_scenes: Dict[str,Scene] = {}
        self.NumEpochs = NumEpochs
        if ScenesPerBatch >0:
            self.ScenesPerBatch = ScenesPerBatch
        else:
            self.ScenesPerBatch = len(scenes)

    from DeepSDFStruct.deep_sdf.networks.deep_sdf_decoder import DeepSDFDecoder

    

    def trainModel(self
    ):
        """
        Train a DeepSDF model on SDFs with compatibility to interpolate between SDFs.
        Operator codes are provided optionally as an attempt to encode multiple functions in a scene.
        Scenes and operators for a scene are wrapped to contain additional input parameters beyond xyz.
        The only condition for compatibility is that all scenes and operators must have the same number of additional parameters.
        Additionally the model should only be trained on watertight smooth shapes for best results.
        If only one opertor is provided per scene the operator code input is omitted.

        """

        # ------------- derive global param space from scenes -------------
        global_add_param_ranges = []
        global_add_num_params = []

        for scene_name, scene in self.scenes.items():
            scene_param_ranges = []
            for op_params in scene.values():
                if isinstance(op_params, (list, tuple)):
                    for rng in op_params:
                        if isinstance(rng, (list, tuple)) and len(rng) == 2:
                            lo, hi = rng
                            if hi >= lo:
                                scene_param_ranges.append((lo, hi))
                            else: 
                                scene_param_ranges.append ((hi,lo))
            
            scene_add_n_params = len(scene_param_ranges)/len(scene.values())
            if scene_add_n_params != int(scene_add_n_params):
                raise ValueError("Inconsistent number of parameters across operators in scene" + scene_name)
            
            if len(scene.values()) >1: scene_add_n_params +=1 

            for i in range(0,len(scene_param_ranges)-1):
                if (scene_param_ranges[i] != scene_param_ranges[i+1]):
                    warnings.warn(f"Inconsistent range of parameters between operators : {i} and {i+1} probably not the best for training")
                
            global_add_param_ranges.append(scene_param_ranges)
            global_add_num_params.append(int(scene_add_n_params))

        for i in range(0,len(global_add_param_ranges)-1):
            if (global_add_param_ranges[i].__len__() != global_add_param_ranges[i+1].__len__()):
                raise ValueError(f"Inconsistent number of parameters between scenes : {list(self.scenes.keys())[i]} and {list(self.scenes.keys())[i+1]} probably not the best for training")
            
            if (global_add_param_ranges[i] != global_add_param_ranges[i+1]):
                warnings.warn(f"Inconsistent range of parameters between scenes :  {list(self.scenes.keys())[i]} and {list(self.scenes.keys())[i+1]} probably not the best for training")
        
        geom_n_params = global_add_num_params[0] +3 # all scenes must have same number of params

        # Define subdirectories consistently
        split_dir = os.path.join(self.base_directory, "split")
        model_params_dir = os.path.join(self.base_directory, "ModelParameters")
        
        samples_dir = os.path.join(self.base_directory, "SdfSamples")
        top_latent_dir = os.path.join(self.base_directory, "LatentCodes")
        optimizer_params_dir = os.path.join(self.base_directory, "OptimizerParameters")

        # Create them if needed
        for d in [self.base_directory, split_dir, model_params_dir, samples_dir, top_latent_dir]:
            os.makedirs(d, exist_ok=True)
    
        print(f"[DEBUG] Using experiment directory: {self.base_directory}")
        print(f"[DEBUG] Latent code directory: {top_latent_dir}")

        # ---------------- Load latest top-level latent codes for resume ----------------
        existing_ckpts = [
            f for f in os.listdir(top_latent_dir) if f.endswith(".pth") and f[:-4].isdigit()
        ]
        latest_epoch = max([int(f[:-4]) for f in existing_ckpts], default=0)

        top_data = {"latent_codes": {}}
        if existing_ckpts:
            top_ckpt_file = os.path.join(top_latent_dir, f"{latest_epoch}.pth")
            top_data = torch.load(top_ckpt_file, map_location="cpu")
            print(f"[DEBUG] Loaded top-level latent codes from {top_ckpt_file}, epoch {latest_epoch}")
        else:
            print(f"[DEBUG] No top-level latent codes found, starting fresh.")

        # ---------------- Specs ----------------
        specs_path = os.path.join(self.base_directory, "specs.json")
        if os.path.exists(specs_path):
            with open(specs_path) as f:
                specs = json.load(f)
        else:
            specs = {
                "Description": f"Train DeepSDF on analytic {self.model_name} shapes.",
                "NetworkArch": "deep_sdf_decoder",
                "DataSource": self.base_directory,
                "TrainSplit": os.path.join(self.base_directory, "split", "TrainSplit.json"),
                "NetworkSpecs": {
                    "dims": [128]*6,
                    "dropout": list(range(6)),
                    "dropout_prob": 0.2,
                    "norm_layers": list(range(6)),
                    "latent_in": [2] if self.latentDim > 0 else [],
                    "xyz_in_all": False,
                    "use_tanh": False,
                    "latent_dropout": False,
                    "weight_norm": True,
                    "geom_dimension": geom_n_params
                },
                "CodeLength": self.latentDim,
                "NumEpochs": self.NumEpochs,
                "SnapshotFrequency": 1,
                "AdditionalSnapshots": [1, 5],
                "LearningRateSchedule": [
                    {"Type": "Step", "Initial": 0.001, "Interval": 250, "Factor": 0.5},
                    {"Type": "Constant", "Value": 0.001}
                ],
                "SamplesPerScene": 5000,
                "ScenesPerBatch": self.ScenesPerBatch,
                "DataLoaderThreads": 1,
                "ClampingDistance": 0.1,
                "CodeRegularization": True,
                "CodeRegularizationLambda": 1e-4,
                "CodeBound": 1.0
            }

        if self.FORCE_ONLY_FINAL_SNAPSHOT:
            specs["SnapshotFrequency"] = specs["NumEpochs"]
            specs["AdditionalSnapshots"] = [specs["NumEpochs"]]

        with open(specs_path, "w") as f:
            json.dump(specs, f, indent=2)

        # ---------------- TrainSplit.json ----------------
        train_split_path = os.path.join(split_dir, "TrainSplit.json")
        if os.path.exists(train_split_path):
            with open(train_split_path) as f:
                split_dict = json.load(f)
        else:
            split_dict = {"train": {}}
        split_dict.setdefault("train", {}).setdefault(self.model_name, [])
        for scene_id in self.scenes.keys():
            #key = f"{self.model_name.lower()}_{scene_id:03d}"

            key = f"{self.model_name.lower()}_{scene_id}"

            if key not in split_dict["train"][self.model_name]:
                split_dict["train"][self.model_name].append(key)
        with open(train_split_path, "w") as f:
            json.dump(split_dict, f, indent=2)

        for scene_idx, (scene_id, scene_with_operators) in enumerate(self.scenes.items()):
            scene_key = f"{self.model_name.lower()}_{scene_id}"

            # --- SDF Samples---
            samples_file = os.path.join(samples_dir, f"{scene_key}.npz")
            if not os.path.exists(samples_file):

                # ---------------- Operator setup ----------------
                operator_keys = list(scene_with_operators.keys())
                random.shuffle(operator_keys)  # shuffle operators
                n_ops = len(operator_keys)

                param_ranges_flat = global_add_param_ranges[scene_idx]
                add_n_params_not_operator = global_add_num_params[scene_idx] -1 if n_ops >1 else global_add_num_params[scene_idx]

                # ---------------- Adaptive sampling per operator ----------------
                # Attempts to sample 50/50 split of inside/outside points per operator

                target_pos = target_neg = specs["SamplesPerScene"] // n_ops// 2
                pos_list, neg_list = [], []

                batch_size = 5000  # sample in batches to avoid memory blow-up
                max_attempts = 1000  # safety to avoid infinite loops

                for i, key in enumerate(operator_keys):
                    op_idx = list(scene_with_operators.keys()).index(key)  # find the index in param_ranges_flat

                    #if no parameter ranges for this operator â†’ skip param sampling
                    if len(param_ranges_flat) == 0:
                        # sample xyz
                        xyz = (torch.rand(batch_size, 3, dtype=torch.float32) * 2 - 1) * self.domainRadius

                        # assemble query depending on single vs multi op
                        if len(operator_keys) > 1:
                            op_value = torch.full((batch_size, 1), float(i), dtype=torch.float32)
                            queries_op = torch.cat([xyz, op_value], dim=1)
                        else:
                            queries_op = xyz

                        # evaluate SDF
                        sdf_fn, _ = scene_with_operators[key]
                        sdf_op_vals = sdf_fn(xyz, None)
                        if sdf_op_vals.dim() == 1:
                            sdf_op_vals = sdf_op_vals.unsqueeze(1)

                        data = torch.cat([queries_op, sdf_op_vals], dim=1).numpy()
                        sdf_col_idx = data.shape[1] - 1
                        batch_pos = data[np.abs(data[:, sdf_col_idx]) < specs["ClampingDistance"]]
                        batch_neg = data[np.abs(data[:, sdf_col_idx]) >= specs["ClampingDistance"]]

                        if len(pos_list) < target_pos:
                            remaining = target_pos - len(pos_list)
                            pos_list.append(batch_pos[:remaining])
                        if len(neg_list) < target_neg:
                            remaining = target_neg - len(neg_list)
                            neg_list.append(batch_neg[:remaining])

                        continue  #skip the parameter sampling path completely

                    # ---------- normal param case ----------
                    low_vals = torch.tensor([lo for lo, _ in param_ranges_flat[op_idx]], dtype=torch.float32)
                    high_vals = torch.tensor([hi for _, hi in param_ranges_flat[op_idx]], dtype=torch.float32)

                    attempts = 0
                    while (len(pos_list) < target_pos or len(neg_list) < target_neg) and attempts < max_attempts:
                        attempts += 1

                        # sample xyz
                        xyz = (torch.rand(batch_size, 3, dtype=torch.float32) * 2 - 1) * self.domainRadius

                        # sample operator parameters
                        rand_params = torch.rand((batch_size, add_n_params_not_operator), dtype=torch.float32)
                        sampled_params = low_vals + rand_params * (high_vals - low_vals)

                        # continuous operator code (column 4)
                        op_value = torch.full((batch_size, 1), float(i), dtype=torch.float32)

                        # assemble query: [x, y, z, op_code, params...]
                        queries_op = torch.cat([xyz, op_value, sampled_params], dim=1)

                        # evaluate SDF
                        sdf_fn, _ = scene_with_operators[key]
                        sdf_op_vals = sdf_fn(xyz, sampled_params)
                        if sdf_op_vals.dim() == 1:
                            sdf_op_vals = sdf_op_vals.unsqueeze(1)

                        data = torch.cat([queries_op, sdf_op_vals], dim=1).numpy()
                        sdf_col_idx = data.shape[1] - 1
                        batch_pos = data[np.abs(data[:, sdf_col_idx]) < specs["ClampingDistance"]]
                        batch_neg = data[np.abs(data[:, sdf_col_idx]) >= specs["ClampingDistance"]]

                        if len(pos_list) < target_pos:
                            remaining = target_pos - len(pos_list)
                            pos_list.append(batch_pos[:remaining])
                        if len(neg_list) < target_neg:
                            remaining = target_neg - len(neg_list)
                            neg_list.append(batch_neg[:remaining])

                # ---------------- Combine all batches ----------------
                pos = np.vstack(pos_list) if pos_list else np.empty((0, batch_size), dtype=np.float32)
                neg = np.vstack(neg_list) if neg_list else np.empty((0, batch_size), dtype=np.float32)

                ## Determine if all scenes have exactly one operator
                all_scenes_single_operator = all(len(scene) == 1 for scene in self.scenes.values())

                # BEFORE saving - ensure we keep the sdf column (last column)
                if all_scenes_single_operator:
                    # pos/neg may have several forms:
                    # - [x,y,z,sdf]                 -> shape[1] == 4  (leave as-is)
                    # - [x,y,z,op_code,params...,sdf] -> shape[1] >= 5 (drop op_code at index 3)
                    # - if somehow other shapes appear, print shapes for debugging
                    if pos.size > 0:
                        if pos.shape[1] >= 5:
                            # drop only the op_code (index 3)
                            pos = np.concatenate([pos[:, :3], pos[:, 4:]], axis=1)
                        elif pos.shape[1] == 4:
                            # already [x,y,z,sdf] -> keep as-is
                            pass
                        else:
                            print(f"[WARN] Unexpected pos shape before saving: {pos.shape}")
                    if neg.size > 0:
                        if neg.shape[1] >= 5:
                            neg = np.concatenate([neg[:, :3], neg[:, 4:]], axis=1)
                        elif neg.shape[1] == 4:
                            pass
                        else:
                            print(f"[WARN] Unexpected neg shape before saving: {neg.shape}")

                # ---------------- Save ----------------
                np.savez_compressed(samples_file, pos=pos, neg=neg)

        # ---------------- Training (Single Pass Over All Scenes) ----------------
        print(f"[INFO] Starting unified DeepSDF training for all {len(self.scenes)} scenes.")

        # Find if we have an existing checkpoint
        existing_model_ckpts = [
            f for f in os.listdir(model_params_dir) if f.endswith(".pth") and f[:-4].isdigit()
        ]
        existing_latent_ckpts = [
            f for f in os.listdir(top_latent_dir) if f.endswith(".pth") and f[:-4].isdigit()
        ]
        latest_epoch = max([int(f[:-4]) for f in existing_model_ckpts], default=0)
        resume_ckpt = str(latest_epoch) if existing_model_ckpts else None

        if resume_ckpt:
            print(f"[INFO] Resuming from epoch {latest_epoch}")
        else:
            print("[INFO] Starting training from scratch")

        # Initialize latent codes if not found
        if not existing_latent_ckpts:
            sigma = 1.0 / math.sqrt(self.latentDim)
            init_latents = torch.normal(0.0, sigma, size=(len(self.scenes), self.latentDim))
            init_latent_dict = {
                "latent_codes": {"weight": init_latents},
                "epoch": 0
            }
            torch.save(init_latent_dict, os.path.join(top_latent_dir, "0.pth"))
            print(f"[INFO] Initialized new latent code matrix for {len(self.scenes)} scenes.")

        # Create dummy logs if needed (DeepSDF expects it)
        logs_file = os.path.join(self.base_directory, "Logs.pth")
        if not os.path.exists(logs_file):
            torch.save({
                "loss": [0.0],
                "learning_rate": [0.001],
                "timing": [0.0],
                "latent_magnitude": [0.0],
                "param_magnitude": {"dummy": [0.0]},
                "epoch": [latest_epoch],
            }, logs_file)

        # Run DeepSDF once for all scenes
        old_cwd = os.getcwd()
        os.chdir(self.base_directory)
        try:
            training.train_deep_sdf(
                experiment_directory=self.base_directory,
                data_source=self.base_directory,
                continue_from=resume_ckpt,
                batch_split=1
            )
        finally:
            os.chdir(old_cwd)

        # ---------------- Load final trained latents ----------------
        post_ckpts = [
            f for f in os.listdir(top_latent_dir) if f.endswith(".pth") and f[:-4].isdigit()
        ]
        if not post_ckpts:
            raise RuntimeError(f"No latent checkpoints found in {top_latent_dir}")
        final_epoch = max(int(f[:-4]) for f in post_ckpts)
        top_latent_path = os.path.join(top_latent_dir, f"{final_epoch}.pth")

        latent_data = torch.load(top_latent_path, map_location="cpu")
        latent_weight = latent_data["latent_codes"]["weight"]

        print(f"[INFO] Loaded final latent codes from epoch {final_epoch}: shape = {latent_weight.shape}")

        # ---------------- Register trained scenes ----------------
        for idx, scene_id in enumerate(self.scenes.keys()):
            scene_key = f"{self.model_name.lower()}_{scene_id}"
            latent_vec = latent_weight[idx]
            self.trained_scenes[scene_key] = Scene(
                parent_model=self,
                scene_key=scene_key,
                latent_vector=latent_vec
            )
        print(f"[INFO] Registered {len(self.trained_scenes)} trained scenes.")


    def get_scene(self, scene_key: str) -> "Scene":
        """Return a Scene instance by key."""
        return self.trained_scenes[scene_key]
    
    def compute_sdf_from_latent(
        self,
        latent_vector: torch.Tensor,
        xyz: torch.Tensor,
        params: Optional[torch.Tensor] = None,
        chunk: int = 50000,
    ):
        """
        Evaluate the trained DeepSDF decoder at xyz locations for a given latent vector.
        Matches the behavior of visualize_a_shape.
        """

        root = self.base_directory
        specs_file = os.path.join(root, "specs.json")

        with open(specs_file, "r") as f:
            specs = json.load(f)

        geom_dim = specs["NetworkSpecs"].get("geom_dimension", 3)

        # ---------------- Load decoder checkpoint ----------------
        model_params_dir = os.path.join(root, "ModelParameters")
        ckpts = [f for f in os.listdir(model_params_dir) if f.endswith(".pth") and f[:-4].isdigit()]
        if not ckpts:
            raise FileNotFoundError(f"No decoder checkpoints found in {model_params_dir}")

        latest_epoch = max(int(f[:-4]) for f in ckpts)
        decoder_path = os.path.join(model_params_dir, f"{latest_epoch}.pth")

        # ---------------- Construct decoder ----------------
        decoder = Decoder(
            latent_size=specs["CodeLength"],
            dims=specs["NetworkSpecs"]["dims"],
            geom_dimension=geom_dim,
            norm_layers=tuple(specs["NetworkSpecs"].get("norm_layers", ())),
            latent_in=tuple(specs["NetworkSpecs"].get("latent_in", ())),
            weight_norm=specs["NetworkSpecs"].get("weight_norm", False),
            xyz_in_all=specs["NetworkSpecs"].get("xyz_in_all", False),
            use_tanh=specs["NetworkSpecs"].get("use_tanh", False),
        )

        ckpt = torch.load(decoder_path, map_location=xyz.device)
        decoder.load_state_dict(ckpt["model_state_dict"])
        decoder.to(xyz.device).eval()

        # ---------------- Sanitize latent ----------------
        if latent_vector.dim() == 1:
            latent_vector = latent_vector.unsqueeze(0)
        latent_vector = latent_vector.to(xyz.device).float().contiguous()

        # ---------------- Sanitize params ----------------
        if params is not None:
            if params.dim() == 1:
                params = params.unsqueeze(0)
            params = params.float().to(xyz.device).contiguous()

        # ---------------- Chunked SDF evaluation ----------------
        outputs = []
        with torch.no_grad():
            N = xyz.shape[0]
            for i in range(0, N, chunk):
                pts = xyz[i:i + chunk]

                if params is not None:
                    pts = torch.cat([pts, params.expand(pts.size(0), -1)], dim=1)

                latent_repeat = latent_vector.expand(pts.size(0), -1)
                decoder_input = torch.cat([latent_repeat, pts], dim=1)

                sdf_chunk = decoder(decoder_input)
                if sdf_chunk.dim() == 2:
                    sdf_chunk = sdf_chunk[:, 0]
                outputs.append(sdf_chunk)

        return torch.cat(outputs, dim=0)

    
class Scene():
    def __init__(self, parent_model: Model, scene_key: str, latent_vector: torch.Tensor):
        # Call the parent constructor to inherit model attributes
        self.parent_model = parent_model
        self.scene_key = scene_key
        self.latent_vector = latent_vector
        # Derive the raw scene id (everything after first underscore)
        raw_id = "_".join(scene_key.split("_")[1:])

        # Pull the operator dict from the parent model
        self.sdf_ops = self.parent_model.scenes.get(raw_id)
        if self.sdf_ops is None:
            raise KeyError(f"Scene id '{raw_id}' not found in parent model.scenes")

    def compute_sdf(self, xyz: torch.Tensor, params: torch.Tensor | None = None, operator: int = 0) -> torch.Tensor:
        """Compute SDF values for this scene using a given operator and optional parameters."""
            
        if self.sdf_ops is None:
            raise ValueError(f"No SDF operators defined for scene '{self.scene_key}'")

        # Lookup operator
        op_entry = self.sdf_ops.get(operator)
        if op_entry is None:
            raise KeyError(f"Operator index {operator} not found in scene '{self.scene_key}'")

        sdf_fn, _ = op_entry  # unpack (function, param_ranges)

        # Call the function with xyz and params
        try:
            sdf_vals = sdf_fn(xyz, params)
        except Exception as e:
            raise RuntimeError(f"Error evaluating SDF for scene '{self.scene_key}', operator {operator}: {e}")

        # Make sure output is the right shape
        if sdf_vals.dim() == 1:
            sdf_vals = sdf_vals.unsqueeze(1)

        return sdf_vals
            

    def compute_trained_sdf(
        self,
        xyz: torch.Tensor,
        params: Optional[torch.Tensor] = None,
        chunk: int = 50000,
    ):
        """
        Evaluate the trained DeepSDF decoder at xyz locations for this scene's latent vector.
        Matches the behavior of visualize_a_shape.
        """

        return self.parent_model.compute_sdf_from_latent(
            latent_vector=self.latent_vector,
            xyz=xyz,
            params=params,
            chunk=chunk,
        )
    
    def get_latent_vector(self) -> torch.Tensor:
        """Return the latent vector for this scene."""
        return self.latent_vector
        
    def visualize(
        self,
        grid_res: int = 128,
        clamp_dist: float = 0.1,
        param_values: list | None = None,
        save_suffix: str | None = None,
        experiment_root: str | None = None,
        ):
        """
        Visualize this scene using the trained latent vector.

        Args:
            grid_res (int): Grid resolution.
            clamp_dist (float): Clamp distance for SDF values.
            param_values (list of list, optional): Parameter vectors to visualize.
            save_suffix (str, optional): Suffix for saved mesh files.
            experiment_root (str, optional): Root directory for experiments.

        Returns:
            List[trimesh.Trimesh]: Generated meshes.
        """
        # Extract scene_id from the scene_key assuming format: "modelname_###"
        scene_id_str = self.scene_key.split("_")[-1]
        scene_id = int(scene_id_str)

        return visualize_a_shape(
            model_name=self.parent_model.model_name,
            scene_id=scene_id,
            grid_res=grid_res,
            clamp_dist=clamp_dist,
            param_values=param_values,
            latent=self.latent_vector,
            save_suffix=save_suffix,
            experiment_root=experiment_root,
        )