import os
import json
import random
from typing import Callable, Dict, List, Optional, Tuple
import torch
import numpy as np
import DeepSDFStruct.deep_sdf.data as deep_data
import DeepSDFStruct.deep_sdf.training as training
from VisualizeAShape import visualize_a_shape
import multiprocessing
import warnings
import math
from DeepSDFStruct.deep_sdf.networks.deep_sdf_decoder import DeepSDFDecoder as Decoder

# ------------------------
# Limit CPU threads globally
# unique to the  machine. 
# Additionally deepSDFStruct and the original deepSDF support NVDIA GPU acceleration
# this has not been implemented in the script but would be useful
# ------------------------
NUM_CORES = multiprocessing.cpu_count()

# Set environment variables for parallel libraries
os.environ["OMP_NUM_THREADS"] = str(NUM_CORES)
os.environ["MKL_NUM_THREADS"] = str(NUM_CORES)

# Configure PyTorch threading
torch.set_num_threads(NUM_CORES)
torch.set_num_interop_threads(min(4, NUM_CORES // 2))

print(f"Configured PyTorch to use {NUM_CORES} CPU cores.")

# ------------------------
# Patch DeepSDF loader for flat SdfSamples
# ------------------------
def patch_get_instance_filenames():
    def get_instance_filenames(data_source, split):
        npyfiles = []
        for split_name, classes in split.items():
            for class_name, instances in classes.items():
                for instance_name in instances:
                    npyfiles.append(f"{instance_name}.npz")
        return npyfiles
    deep_data.get_instance_filenames = get_instance_filenames

patch_get_instance_filenames()

# ------------------------
# Caution! 
# Upon resuming training this script generates log.pth files to satisfy DeepSDF's API. 
#
# the log.pth generated by the script are dummy files and do not contain any logs
#
# Any such useful informtion could be extracted from this script with
# minor modifications however. 
# ------------------------
SDFCallable = Callable[[torch.Tensor, torch.Tensor | None], torch.Tensor]
SceneWithOperators = Dict[int, Tuple[SDFCallable, List[Tuple[float, float]]]]
Scenes = Dict[str, SceneWithOperators]

class Model: 
    def __init__(
        self,
        base_directory: str,
        model_name: str,
        scenes: Scenes,
        resume: bool = True,
        domainRadius: float = 1.0,
        latentDim: int = 1,
        FORCE_ONLY_FINAL_SNAPSHOT: bool = False,
        NumEpochs: int =500,
        ScenesPerBatch: int = 0,
    ):
        self.base_directory = base_directory
        self.model_name = model_name
        self.scenes = scenes
        self.resume = resume
        self.domainRadius = domainRadius
        self.latentDim = latentDim
        self.FORCE_ONLY_FINAL_SNAPSHOT = FORCE_ONLY_FINAL_SNAPSHOT
        self.trained_scenes: Dict[str,Scene] = {}
        self.NumEpochs = NumEpochs
        if ScenesPerBatch >0:
            self.ScenesPerBatch = ScenesPerBatch
        else:
            self.ScenesPerBatch = len(scenes)

    def trainModel(self
    ):
        """
        Train a DeepSDF model on SDFs with compatibility to interpolate between SDFs.
        Operator codes are provided optionally as an attempt to encode multiple functions in a scene.
        Scenes and operators for a scene are wrapped to contain additional input parameters beyond xyz.
        The only condition for compatibility is that all scenes and operators must have the same number of additional parameters.
        Additionally the model should only be trained on watertight smooth shapes for best results.
        If only one opertor is provided per scene the operator code input is omitted.

        """

        # ------------- derive global param space from scenes -------------
        global_add_param_ranges = []
        global_add_num_params = []

        for scene_name, scene in self.scenes.items():
            scene_param_ranges = []
            for op_params in scene.values():
                if isinstance(op_params, (list, tuple)):
                    for rng in op_params:
                        if isinstance(rng, (list, tuple)) and len(rng) == 2:
                            lo, hi = rng
                            if hi >= lo:
                                scene_param_ranges.append((lo, hi))
                            else: 
                                scene_param_ranges.append ((hi,lo))
            
            scene_add_n_params = len(scene_param_ranges)/len(scene.values())
            if scene_add_n_params != int(scene_add_n_params):
                raise ValueError("Inconsistent number of parameters across operators in scene" + scene_name)
            
            if len(scene.values()) >1: scene_add_n_params +=1 

            for i in range(0,len(scene_param_ranges)-1):
                if (scene_param_ranges[i] != scene_param_ranges[i+1]):
                    warnings.warn(f"Inconsistent range of parameters between operators : {i} and {i+1} probably not the best for training")
                
            global_add_param_ranges.append(scene_param_ranges)
            global_add_num_params.append(int(scene_add_n_params))

        for i in range(0,len(global_add_param_ranges)-1):
            if (global_add_param_ranges[i].__len__() != global_add_param_ranges[i+1].__len__()):
                raise ValueError(f"Inconsistent number of parameters between scenes : {list(self.scenes.keys())[i]} and {list(self.scenes.keys())[i+1]} probably not the best for training")
            
            if (global_add_param_ranges[i] != global_add_param_ranges[i+1]):
                warnings.warn(f"Inconsistent range of parameters between scenes :  {list(self.scenes.keys())[i]} and {list(self.scenes.keys())[i+1]} probably not the best for training")
        
        geom_n_params = global_add_num_params[0] +3 # all scenes must have same number of params

        # Define subdirectories consistently
        split_dir = os.path.join(self.base_directory, "split")
        model_params_dir = os.path.join(self.base_directory, "ModelParameters")
        
        samples_dir = os.path.join(self.base_directory, "SdfSamples")
        top_latent_dir = os.path.join(self.base_directory, "LatentCodes")
        optimizer_params_dir = os.path.join(self.base_directory, "OptimizerParameters")

        # Create them if needed
        for d in [self.base_directory, split_dir, model_params_dir, samples_dir, top_latent_dir]:
            os.makedirs(d, exist_ok=True)
    
        print(f"[DEBUG] Using experiment directory: {self.base_directory}")
        print(f"[DEBUG] Latent code directory: {top_latent_dir}")

        # ---------------- Load latest top-level latent codes for resume ----------------
        existing_ckpts = [
            f for f in os.listdir(top_latent_dir) if f.endswith(".pth") and f[:-4].isdigit()
        ]
        latest_epoch = max([int(f[:-4]) for f in existing_ckpts], default=0)

        top_data = {"latent_codes": {}}
        if existing_ckpts:
            top_ckpt_file = os.path.join(top_latent_dir, f"{latest_epoch}.pth")
            top_data = torch.load(top_ckpt_file, map_location="cpu")
            print(f"[DEBUG] Loaded top-level latent codes from {top_ckpt_file}, epoch {latest_epoch}")
        else:
            print(f"[DEBUG] No top-level latent codes found, starting fresh.")

        # ---------------- Specs ----------------
        specs_path = os.path.join(self.base_directory, "specs.json")
        if os.path.exists(specs_path):
            with open(specs_path) as f:
                specs = json.load(f)
        else:
            specs = {
                "Description": f"Train DeepSDF on analytic {self.model_name} shapes.",
                "NetworkArch": "deep_sdf_decoder",
                "DataSource": self.base_directory,
                "TrainSplit": os.path.join(self.base_directory, "split", "TrainSplit.json"),
                "NetworkSpecs": {
                    "dims": [128]*6,
                    "dropout": list(range(6)),
                    "dropout_prob": 0.2,
                    "norm_layers": list(range(6)),
                    "latent_in": [2] if self.latentDim > 0 else [],
                    "xyz_in_all": False,
                    "use_tanh": False,
                    "latent_dropout": False,
                    "weight_norm": True,
                    "geom_dimension": geom_n_params
                },
                "CodeLength": self.latentDim,
                "NumEpochs": self.NumEpochs,
                "SnapshotFrequency": 1,
                "AdditionalSnapshots": [1, 5],
                "LearningRateSchedule": [
                    {"Type": "Step", "Initial": 0.001, "Interval": 250, "Factor": 0.5},
                    {"Type": "Constant", "Value": 0.001}
                ],
                "SamplesPerScene": 50000,
                "ScenesPerBatch": self.ScenesPerBatch,
                "DataLoaderThreads": 1,
                "ClampingDistance": 0.1,
                "CodeRegularization": True,
                "CodeRegularizationLambda": 1e-4,
                "CodeBound": 1.0
            }

        if self.FORCE_ONLY_FINAL_SNAPSHOT:
            specs["SnapshotFrequency"] = specs["NumEpochs"]
            specs["AdditionalSnapshots"] = [specs["NumEpochs"]]

        with open(specs_path, "w") as f:
            json.dump(specs, f, indent=2)

        # ---------------- TrainSplit.json ----------------
        train_split_path = os.path.join(split_dir, "TrainSplit.json")
        if os.path.exists(train_split_path):
            with open(train_split_path) as f:
                split_dict = json.load(f)
        else:
            split_dict = {"train": {}}
        split_dict.setdefault("train", {}).setdefault(self.model_name, [])
        for scene_id in self.scenes.keys():
            #key = f"{self.model_name.lower()}_{scene_id:03d}"

            key = f"{self.model_name.lower()}_{scene_id}"

            if key not in split_dict["train"][self.model_name]:
                split_dict["train"][self.model_name].append(key)
        with open(train_split_path, "w") as f:
            json.dump(split_dict, f, indent=2)

    
        # -------------------------
        # Begin scene sampling loop (robust, deterministic high angular uniformity)
        # -------------------------
        for scene_idx, (scene_id, scene_with_operators) in enumerate(self.scenes.items()):
            instance_key = f"{self.model_name.lower()}_{scene_id}"
            samples_file = os.path.join(samples_dir, f"{instance_key}.npz")

            print("\n" + "=" * 62)
            print(f"[SAMPLE] Begin sampling for scene: {instance_key}")
            print("=" * 62)

            if os.path.exists(samples_file):
                print(f"[SAMPLE] Existing file found, skipping: {samples_file}")
                continue

            operator_keys = list(scene_with_operators.keys())
            random.shuffle(operator_keys)
            n_ops = len(operator_keys)
            param_ranges_flat = global_add_param_ranges[scene_idx]

            cl = specs.get("ClampingDistance", 0.1)
            batch_size = 20000
            max_attempts = 1500
            NUM_AZ, NUM_EL = 36, 18
            NUM_BINS = NUM_AZ * NUM_EL

            # -------------------------
            # Helpers
            # -------------------------
         
            # -------------------------
            # Very high-accuracy center estimation (geometric, gradient-free)
            # -------------------------
            def estimate_center(sdf_fn, fallback=torch.zeros(3, dtype=torch.float32), probe_N=100000):
                """
                Estimate the geometric center of a shape by projecting near-surface points to the SDF zero level.
                Works extremely well for symmetric shapes like spheres.
                """
                # 1. Sample points uniformly in the domain
                probe_pts = (torch.rand(probe_N, 3) * 2 - 1) * self.domainRadius
                sdf_vals = sdf_fn(probe_pts, None)
                if sdf_vals.dim() == 2:
                    sdf_vals = sdf_vals[:, 0]

                # 2. Keep only points near the surface
                mask = torch.abs(sdf_vals) < max(0.3, cl*3)
                if mask.sum() < 128:
                    print("[WARN] Not enough near-surface points, falling back to origin")
                    return fallback

                near_pts = probe_pts[mask]
                near_sdf = sdf_vals[mask]

                # 3. Project points toward zero level using SDF values along vector from origin
                projected_pts = near_pts - near_sdf.unsqueeze(1) * (near_pts / (near_pts.norm(dim=1, keepdim=True) + 1e-12))

                # 4. Coordinate-wise median is extremely robust
                center_est = projected_pts.median(dim=0).values

                # 5. Optional: refine locally with smaller random perturbation around estimated center
                local_N = min(20000, probe_N)
                local_pts = center_est + (torch.rand(local_N, 3) * 2 - 1) * self.domainRadius * 0.1
                sdf_local = sdf_fn(local_pts, None)
                if sdf_local.dim() == 2:
                    sdf_local = sdf_local[:, 0]
                mask_local = torch.abs(sdf_local) < max(0.3, cl*3)
                if mask_local.sum() > 64:
                    projected_local = local_pts[mask_local] - sdf_local[mask_local].unsqueeze(1) * \
                                    (local_pts[mask_local] / (local_pts[mask_local].norm(dim=1, keepdim=True) + 1e-12))
                    center_est = projected_local.median(dim=0).values

                return center_est



            def sample_uniform_dirs_torch(n):
                v = torch.randn(n,3)
                return v / (v.norm(dim=1, keepdim=True)+1e-12)

            def dirs_to_bins_numpy(dirs_np):
                x,y,z = dirs_np[:,0], dirs_np[:,1], dirs_np[:,2]
                az = (np.arctan2(y,x)+np.pi)/(2*np.pi)
                el = (np.arcsin(np.clip(z,-1,1))+np.pi/2)/np.pi
                az_idx = np.clip(np.floor(az*NUM_AZ).astype(int),0,NUM_AZ-1)
                el_idx = np.clip(np.floor(el*NUM_EL).astype(int),0,NUM_EL-1)
                flat = az_idx*NUM_EL + el_idx
                return flat, az_idx, el_idx

            def uniformity_score_from_counts(counts):
                counts = counts.astype(np.float32)
                return 1.0 / (1.0 + counts.std()/(counts.mean()+1e-9)) if counts.sum()>0 else 0.0

            def estimate_surface_radius(sdf_fn, center, num_probes=2048):
                dirs = sample_uniform_dirs_torch(num_probes)
                probe_pts = center.unsqueeze(0) + dirs * (self.domainRadius*0.95)
                sdf_vals = sdf_fn(probe_pts, None)
                if sdf_vals.dim()==2: sdf_vals = sdf_vals[:,0]
                approx_Rs = ((probe_pts - center.unsqueeze(0)).norm(dim=1) - sdf_vals).cpu().numpy()
                R = float(np.median(approx_Rs))
                return R if np.isfinite(R) and R>0 else float(self.domainRadius*0.9)

            any_sdf_fn, _ = next(iter(scene_with_operators.values()))
            shape_center = estimate_center(any_sdf_fn).float()
            print(f"[INFO] Estimated center: {shape_center.tolist()}")

            # -------------------------
            # Per-operator targets
            # -------------------------
            total_target_each_op = max(1, specs["SamplesPerScene"] // max(1,n_ops))
            target_pos = total_target_each_op // 2
            target_neg = total_target_each_op - target_pos

            pos_chunks, neg_chunks = [], []
            op_stats = {k: {"pos":0,"neg":0,"attempts":0,"pos_uniformity":0.0,"neg_uniformity":0.0,
                            "pos_bin_counts":[],"neg_bin_counts":[]} for k in operator_keys}

            # -------------------------
            # Per-operator sampling
            # -------------------------
            for op_idx, key in enumerate(operator_keys):
                sdf_fn, _ = scene_with_operators[key]
                op_has_params = len(param_ranges_flat) > op_idx and len(param_ranges_flat[op_idx])>0
                if op_has_params:
                    pr = param_ranges_flat[op_idx]
                    low_vals = torch.tensor([lo for lo,_ in pr], dtype=torch.float32)
                    high_vals = torch.tensor([hi for _,hi in pr], dtype=torch.float32)
                    n_params_for_op = len(pr)
                else:
                    low_vals = high_vals = None
                    n_params_for_op = 0

                surface_radius = estimate_surface_radius(sdf_fn, shape_center)
                shell_min = max(0.0, surface_radius - cl*1.5)
                shell_max = surface_radius + cl*1.5

                # bin tracking
                pos_bin_counts = np.zeros(NUM_BINS,dtype=int)
                neg_bin_counts = np.zeros(NUM_BINS,dtype=int)
                bin_target_pos = max(1,target_pos//NUM_BINS)
                bin_target_neg = max(1,target_neg//NUM_BINS)

                op_pos_buf, op_neg_buf = [], []
                attempts = 0

                # -------------------------
                # Deterministic per-bin sampling
                # -------------------------
                while (sum(c.shape[0] for c in op_pos_buf) < target_pos or sum(c.shape[0] for c in op_neg_buf) < target_neg) and attempts<max_attempts:
                    attempts += 1
                    dirs = sample_uniform_dirs_torch(batch_size)
                    r = shell_min + torch.rand(batch_size,1)*(shell_max-shell_min)
                    xyz = shape_center.unsqueeze(0)+dirs*r

                    if op_has_params:
                        rp = torch.rand(batch_size,n_params_for_op)
                        sampled_params = low_vals + rp*(high_vals-low_vals)
                    else:
                        sampled_params = None

                    sdf_vals = sdf_fn(xyz, sampled_params)
                    if sdf_vals.dim()==1: sdf_vals = sdf_vals.unsqueeze(1)
                    data = torch.cat([xyz, sdf_vals], dim=1).cpu().numpy()
                    sdf_v = data[:,-1]

                    mask_near = np.abs(sdf_v)<=cl
                    if mask_near.sum()==0: continue
                    near_data = data[mask_near]
                    near_xyz = near_data[:,:3]
                    near_sdf = near_data[:,-1]
                    is_pos = near_sdf>=0
                    is_neg = ~is_pos

                    rel = near_xyz - shape_center.cpu().numpy()[None,:]
                    norms = np.linalg.norm(rel,axis=1,keepdims=True)+1e-12
                    dirs_np = rel/norms
                    flat_idx, _, _ = dirs_to_bins_numpy(dirs_np)

                    # -------------------------
                    # Deterministic selection per bin
                    # -------------------------
                    pos_selected = []
                    neg_selected = []
                    for bin_id in range(NUM_BINS):
                        bin_pos_idxs = np.where(is_pos & (flat_idx==bin_id))[0]
                        bin_neg_idxs = np.where(is_neg & (flat_idx==bin_id))[0]

                        n_select_pos = max(0, bin_target_pos - pos_bin_counts[bin_id])
                        n_select_neg = max(0, bin_target_neg - neg_bin_counts[bin_id])

                        if len(bin_pos_idxs)>0 and n_select_pos>0:
                            chosen = np.random.choice(bin_pos_idxs, min(n_select_pos,len(bin_pos_idxs)), replace=False)
                            pos_selected.append(near_data[chosen])
                            pos_bin_counts[bin_id] += len(chosen)

                        if len(bin_neg_idxs)>0 and n_select_neg>0:
                            chosen = np.random.choice(bin_neg_idxs, min(n_select_neg,len(bin_neg_idxs)), replace=False)
                            neg_selected.append(near_data[chosen])
                            neg_bin_counts[bin_id] += len(chosen)

                    if pos_selected: op_pos_buf.append(np.vstack(pos_selected))
                    if neg_selected: op_neg_buf.append(np.vstack(neg_selected))

                    op_stats[key]["pos"] = sum(c.shape[0] for c in op_pos_buf)
                    op_stats[key]["neg"] = sum(c.shape[0] for c in op_neg_buf)
                    op_stats[key]["attempts"] = attempts

                pos_chunks.append(np.vstack(op_pos_buf) if op_pos_buf else np.empty((0, data.shape[1])))
                neg_chunks.append(np.vstack(op_neg_buf) if op_neg_buf else np.empty((0, data.shape[1])))
                op_stats[key]["pos_uniformity"] = uniformity_score_from_counts(pos_bin_counts)
                op_stats[key]["neg_uniformity"] = uniformity_score_from_counts(neg_bin_counts)
                op_stats[key]["pos_bin_counts"] = pos_bin_counts.tolist()
                op_stats[key]["neg_bin_counts"] = neg_bin_counts.tolist()

            # -------------------------
            # Merge all operators and save
            # -------------------------
            pos = np.vstack(pos_chunks) if any(p.size for p in pos_chunks) else np.empty((0,data.shape[1]),dtype=np.float32)
            neg = np.vstack(neg_chunks) if any(n.size for n in neg_chunks) else np.empty((0,data.shape[1]),dtype=np.float32)

            if all(len(scene)==1 for scene in self.scenes.values()):
                if pos.shape[1]>3: pos = np.concatenate([pos[:,:3], pos[:,4:]], axis=1)
                if neg.shape[1]>3: neg = np.concatenate([neg[:,:3], neg[:,4:]], axis=1)

            pos = pos.astype(np.float32)
            neg = neg.astype(np.float32)
            min_cols = min(pos.shape[1], neg.shape[1])
            if pos.shape[1]!=neg.shape[1]:
                print("[WARN] Pos/Neg column mismatch; trimming to:", min_cols)
                pos = pos[:,:min_cols]
                neg = neg[:,:min_cols]

            print(f"[SAVE] Writing samples to: {samples_file}")
            np.savez_compressed(samples_file,pos=pos,neg=neg)

            print("\n[OPERATOR STATISTICS for scene]")
            for k in operator_keys:
                s = op_stats[k]
                total_seen = s.get("pos",0)+s.get("neg",0)
                print(f"  {k}: attempts={s.get('attempts',0)}, total_seen={total_seen}, pos={s.get('pos',0)}, neg={s.get('neg',0)}")
                print(f"    uniformity -> pos={s.get('pos_uniformity',0):.4f}, neg={s.get('neg_uniformity',0):.4f}")

            print("="*62)
            print(f"[SAMPLE] Completed scene: {instance_key}")
            print("="*62)
        # -------------------------
        # End scene sampling loop
        # -------------------------


        # ---------------- Training (Single Pass Over All Scenes) ----------------
        print(f"[INFO] Starting unified DeepSDF training for all {len(self.scenes)} scenes.")

        # Find if we have an existing checkpoint
        existing_model_ckpts = [
            f for f in os.listdir(model_params_dir) if f.endswith(".pth") and f[:-4].isdigit()
        ]
        existing_latent_ckpts = [
            f for f in os.listdir(top_latent_dir) if f.endswith(".pth") and f[:-4].isdigit()
        ]
        latest_epoch = max([int(f[:-4]) for f in existing_model_ckpts], default=0)
        resume_ckpt = str(latest_epoch) if existing_model_ckpts else None

        if resume_ckpt:
            print(f"[INFO] Resuming from epoch {latest_epoch}")
        else:
            print("[INFO] Starting training from scratch")

        # Initialize latent codes if not found
        if not existing_latent_ckpts:
            sigma = 1.0 / math.sqrt(self.latentDim)
            init_latents = torch.normal(0.0, sigma, size=(len(self.scenes), self.latentDim))
            init_latent_dict = {
                "latent_codes": {"weight": init_latents},
                "epoch": 0
            }
            torch.save(init_latent_dict, os.path.join(top_latent_dir, "0.pth"))
            print(f"[INFO] Initialized new latent code matrix for {len(self.scenes)} scenes.")

        # Create dummy logs if needed (DeepSDF expects it)
        logs_file = os.path.join(self.base_directory, "Logs.pth")
        if not os.path.exists(logs_file):
            torch.save({
                "loss": [0.0],
                "learning_rate": [0.001],
                "timing": [0.0],
                "latent_magnitude": [0.0],
                "param_magnitude": {"dummy": [0.0]},
                "epoch": [latest_epoch],
            }, logs_file)

        # Run DeepSDF once for all scenes
        old_cwd = os.getcwd()
        os.chdir(self.base_directory)
        try:
            training.train_deep_sdf(
                experiment_directory=self.base_directory,
                data_source=self.base_directory,
                continue_from=resume_ckpt,
                batch_split=1
            )
        finally:
            os.chdir(old_cwd)

        # ---------------- Load final trained latents ----------------
        post_ckpts = [
            f for f in os.listdir(top_latent_dir) if f.endswith(".pth") and f[:-4].isdigit()
        ]
        if not post_ckpts:
            raise RuntimeError(f"No latent checkpoints found in {top_latent_dir}")
        final_epoch = max(int(f[:-4]) for f in post_ckpts)
        top_latent_path = os.path.join(top_latent_dir, f"{final_epoch}.pth")

        latent_data = torch.load(top_latent_path, map_location="cpu")
        latent_weight = latent_data["latent_codes"]["weight"]

        print(f"[INFO] Loaded final latent codes from epoch {final_epoch}: shape = {latent_weight.shape}")

        # ---------------- Register trained scenes ----------------
        for idx, scene_id in enumerate(self.scenes.keys()):
            scene_key = f"{self.model_name.lower()}_{scene_id}"
            latent_vec = latent_weight[idx]
            self.trained_scenes[scene_key] = Scene(
                parent_model=self,
                scene_key=scene_key,
                latent_vector=latent_vec
            )
        print(f"[INFO] Registered {len(self.trained_scenes)} trained scenes.")


    def get_scene(self, scene_key: str) -> "Scene":
        """Return a Scene instance by key."""
        return self.trained_scenes[scene_key]
    
    def compute_sdf_from_latent(
        self,
        latent_vector: torch.Tensor,
        xyz: torch.Tensor,
        params: Optional[torch.Tensor] = None,
        chunk: int = 50000,
    ):
        """
        Evaluate the trained DeepSDF decoder at xyz locations for a given latent vector.
        Matches the behavior of visualize_a_shape.
        """

        root = self.base_directory
        specs_file = os.path.join(root, "specs.json")

        with open(specs_file, "r") as f:
            specs = json.load(f)

        geom_dim = specs["NetworkSpecs"].get("geom_dimension", 3)

        # ---------------- Load decoder checkpoint ----------------
        model_params_dir = os.path.join(root, "ModelParameters")
        ckpts = [f for f in os.listdir(model_params_dir) if f.endswith(".pth") and f[:-4].isdigit()]
        if not ckpts:
            raise FileNotFoundError(f"No decoder checkpoints found in {model_params_dir}")

        latest_epoch = max(int(f[:-4]) for f in ckpts)
        decoder_path = os.path.join(model_params_dir, f"{latest_epoch}.pth")

        # ---------------- Construct decoder ----------------
        decoder = Decoder(
            latent_size=specs["CodeLength"],
            dims=specs["NetworkSpecs"]["dims"],
            geom_dimension=geom_dim,
            norm_layers=tuple(specs["NetworkSpecs"].get("norm_layers", ())),
            latent_in=tuple(specs["NetworkSpecs"].get("latent_in", ())),
            weight_norm=specs["NetworkSpecs"].get("weight_norm", False),
            xyz_in_all=specs["NetworkSpecs"].get("xyz_in_all", False),
            use_tanh=specs["NetworkSpecs"].get("use_tanh", False),
        )

        ckpt = torch.load(decoder_path, map_location=xyz.device)
        decoder.load_state_dict(ckpt["model_state_dict"])
        decoder.to(xyz.device).eval()

        # ---------------- Sanitize latent ----------------
        if latent_vector.dim() == 1:
            latent_vector = latent_vector.unsqueeze(0)
        latent_vector = latent_vector.to(xyz.device).float().contiguous()

        # ---------------- Sanitize params ----------------
        if params is not None:
            if params.dim() == 1:
                params = params.unsqueeze(0)
            params = params.float().to(xyz.device).contiguous()

        # ---------------- Chunked SDF evaluation ----------------
        outputs = []
        with torch.no_grad():
            N = xyz.shape[0]
            for i in range(0, N, chunk):
                pts = xyz[i:i + chunk]

                if params is not None:
                    pts = torch.cat([pts, params.expand(pts.size(0), -1)], dim=1)

                latent_repeat = latent_vector.expand(pts.size(0), -1)
                decoder_input = torch.cat([latent_repeat, pts], dim=1)

                sdf_chunk = decoder(decoder_input)
                if sdf_chunk.dim() == 2:
                    sdf_chunk = sdf_chunk[:, 0]
                outputs.append(sdf_chunk)

        return torch.cat(outputs, dim=0)

    
class Scene():
    def __init__(self, parent_model: Model, scene_key: str, latent_vector: torch.Tensor):
        # Call the parent constructor to inherit model attributes
        self.parent_model = parent_model
        self.scene_key = scene_key
        self.latent_vector = latent_vector
        # Derive the raw scene id (everything after first underscore)
        raw_id = "_".join(scene_key.split("_")[1:])

        # Pull the operator dict from the parent model
        self.sdf_ops = self.parent_model.scenes.get(raw_id)
        if self.sdf_ops is None:
            raise KeyError(f"Scene id '{raw_id}' not found in parent model.scenes")

    def compute_sdf(self, xyz: torch.Tensor, params: torch.Tensor | None = None, operator: int = 0) -> torch.Tensor:
        """Compute SDF values for this scene using a given operator and optional parameters."""
            
        if self.sdf_ops is None:
            raise ValueError(f"No SDF operators defined for scene '{self.scene_key}'")

        # Lookup operator
        op_entry = self.sdf_ops.get(operator)
        if op_entry is None:
            raise KeyError(f"Operator index {operator} not found in scene '{self.scene_key}'")

        sdf_fn, _ = op_entry  # unpack (function, param_ranges)

        # Call the function with xyz and params
        try:
            sdf_vals = sdf_fn(xyz, params)
        except Exception as e:
            raise RuntimeError(f"Error evaluating SDF for scene '{self.scene_key}', operator {operator}: {e}")

        # Make sure output is the right shape
        if sdf_vals.dim() == 1:
            sdf_vals = sdf_vals.unsqueeze(1)

        return sdf_vals
            

    def compute_trained_sdf(
        self,
        xyz: torch.Tensor,
        params: Optional[torch.Tensor] = None,
        chunk: int = 50000,
    ):
        """
        Evaluate the trained DeepSDF decoder at xyz locations for this scene's latent vector.
        Matches the behavior of visualize_a_shape.
        """

        return self.parent_model.compute_sdf_from_latent(
            latent_vector=self.latent_vector,
            xyz=xyz,
            params=params,
            chunk=chunk,
        )
    
    def get_latent_vector(self) -> torch.Tensor:
        """Return the latent vector for this scene."""
        return self.latent_vector
        
    def visualize(
        self,
        grid_res: int = 128,
        clamp_dist: float = 0.1,
        param_values: list | None = None,
        save_suffix: str | None = None,
        experiment_root: str | None = None,
        ):
        """
        Visualize this scene using the trained latent vector.

        Args:
            grid_res (int): Grid resolution.
            clamp_dist (float): Clamp distance for SDF values.
            param_values (list of list, optional): Parameter vectors to visualize.
            save_suffix (str, optional): Suffix for saved mesh files.
            experiment_root (str, optional): Root directory for experiments.

        Returns:
            List[trimesh.Trimesh]: Generated meshes.
        """
        # Extract scene_id from the scene_key assuming format: "modelname_###"
        scene_id_str = self.scene_key.split("_")[-1]
        scene_id = int(scene_id_str)

        return visualize_a_shape(
            model_name=self.parent_model.model_name,
            scene_id=scene_id,
            grid_res=grid_res,
            clamp_dist=clamp_dist,
            param_values=param_values,
            latent=self.latent_vector,
            save_suffix=save_suffix,
            experiment_root=experiment_root,
        )